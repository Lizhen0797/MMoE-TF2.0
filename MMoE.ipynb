{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import tensordot, expand_dims\n",
    "from tensorflow.keras import layers, Model, initializers, regularizers, activations, constraints, Input\n",
    "\n",
    "from tensorflow.keras.backend import expand_dims, repeat_elements, sum\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMoE(tf.keras.Model):\n",
    "    '''\n",
    "    @param units MMoE隐藏层单元数\n",
    "    @param num_experts MMoE专家数\n",
    "    @param num_tasks 下游任务数\n",
    "    @param use_expert_bias 是否用用expert的bias\n",
    "    @param use_gate_bias 是否用用gat的bias\n",
    "    @param expert_activation expert的激活函数\n",
    "    @param gate_activation gated的激活函数\n",
    "    @param expert_bias_initializer expert bias的初始化函数\n",
    "    @param gate_bias_initializer gate bias的初始化函数\n",
    "    @param expert_bias_regularizer expert bias的正则化方式\n",
    "    @param gate_bias_regularizer gate bias的正则化方式\n",
    "    @param expert_bias_constraint expert bias的约束\n",
    "    @param gate_bias_constraint gate bias的约束\n",
    "    @param expert_kernel_initializer expert权重初始化\n",
    "    @param gate_kernel_initializer gate权重初始化\n",
    "    @param expert_kernel_regularizer expert权重正则化\n",
    "    @param gate_kernel_regularizer gate权重正则化\n",
    "    @param expert_kernel_constraint expert权重约束\n",
    "    @param gate_kernel_constraint gate权重约束\n",
    "    @param activity_regularizery activate正则化函数\n",
    "    @param kwargs Layer类附加参数\n",
    "    '''\n",
    "    def __init__(self, units, num_experts, num_tasks, \n",
    "                 use_expert_bias=True,use_gate_bias=True,expert_activation='relu', gate_activation='softmax',\n",
    "                 expert_bias_initializer='zeros',gate_bias_initializer='zeros',expert_bias_regularizer=None, \n",
    "                 gate_bias_regularizer=None, expert_bias_constraint=None,gate_bias_constraint=None,\n",
    "                 expert_kernel_initializer='VarianceScaling', gate_kernel_initializer='VarianceScaling',\n",
    "                 expert_kernel_regularizer=None,gate_kernel_regularizer=None,expert_kernel_constraint=None,\n",
    "                 gate_kernel_constraint=None,activity_regularizer=None, **kwargs):\n",
    "        super(MMoE, self).__init__(**kwargs)\n",
    "        \n",
    "        self.units = units\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # Weight parameter\n",
    "        self.expert_kernels = None\n",
    "        self.gate_kernels = None\n",
    "        self.expert_kernel_initializer = initializers.get(expert_kernel_initializer)\n",
    "        self.gate_kernel_initializer = initializers.get(gate_kernel_initializer)\n",
    "        self.expert_kernel_regularizer = regularizers.get(expert_kernel_regularizer)\n",
    "        self.gate_kernel_regularizer = regularizers.get(gate_kernel_regularizer)\n",
    "        self.expert_kernel_constraint = constraints.get(expert_kernel_constraint)\n",
    "        self.gate_kernel_constraint = constraints.get(gate_kernel_constraint)\n",
    "\n",
    "        # Activation parameter\n",
    "        #self.expert_activation = activations.get(expert_activation)\n",
    "        self.expert_activation = expert_activation\n",
    "        self.gate_activation = gate_activation\n",
    "\n",
    "        # Bias parameter\n",
    "        self.expert_bias = None\n",
    "        self.gate_bias = None\n",
    "        self.use_expert_bias = use_expert_bias\n",
    "        self.use_gate_bias = use_gate_bias\n",
    "        self.expert_bias_initializer = initializers.get(expert_bias_initializer)\n",
    "        self.gate_bias_initializer = initializers.get(gate_bias_initializer)\n",
    "        self.expert_bias_regularizer = regularizers.get(expert_bias_regularizer)\n",
    "        self.gate_bias_regularizer = regularizers.get(gate_bias_regularizer)\n",
    "        self.expert_bias_constraint = constraints.get(expert_bias_constraint)\n",
    "        self.gate_bias_constraint = constraints.get(gate_bias_constraint)\n",
    "\n",
    "        # Activity parameter\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        \n",
    "        self.expert_layers = []\n",
    "        self.gate_layers = []\n",
    "        \n",
    "        for i in range(self.num_experts):\n",
    "            self.expert_layers.append(layers.Dense(self.units, activation=self.expert_activation,\n",
    "                                                   use_bias=self.use_expert_bias,\n",
    "                                                   kernel_initializer=self.expert_kernel_initializer,\n",
    "                                                   bias_initializer=self.expert_bias_initializer,\n",
    "                                                   kernel_regularizer=self.expert_kernel_regularizer,\n",
    "                                                   bias_regularizer=self.expert_bias_regularizer,\n",
    "                                                   activity_regularizer=None,\n",
    "                                                   kernel_constraint=self.expert_kernel_constraint,\n",
    "                                                   bias_constraint=self.expert_bias_constraint))\n",
    "        for i in range(self.num_tasks):\n",
    "            self.gate_layers.append(layers.Dense(self.num_experts, activation=self.gate_activation,\n",
    "                                                 use_bias=self.use_gate_bias,\n",
    "                                                 kernel_initializer=self.gate_kernel_initializer,\n",
    "                                                 bias_initializer=self.gate_bias_initializer,\n",
    "                                                 kernel_regularizer=self.gate_kernel_regularizer,\n",
    "                                                 bias_regularizer=self.gate_bias_regularizer, activity_regularizer=None,\n",
    "                                                 kernel_constraint=self.gate_kernel_constraint,\n",
    "                                                 bias_constraint=self.gate_bias_constraint))\n",
    "    def call(self, inputs):\n",
    "        expert_outputs, gate_outputs, final_outputs = [], [], []\n",
    "        for expert_layer in self.expert_layers:\n",
    "            expert_output = expand_dims(expert_layer(inputs), axis=2)\n",
    "            #print(expert_output.shape)\n",
    "            expert_outputs.append(expert_output)\n",
    "        expert_outputs = tf.concat(expert_outputs,2)\n",
    "        #print(expert_outputs.shape)\n",
    "        \n",
    "        for gate_layer in self.gate_layers:\n",
    "            gate_outputs.append(gate_layer(inputs))\n",
    "            \n",
    "        for gate_output in gate_outputs:\n",
    "            #print('Gate Ouput')\n",
    "            #print(gate_output.shape)\n",
    "            expanded_gate_output = expand_dims(gate_output, axis=1)\n",
    "            #print(expanded_gate_output.shape)\n",
    "            weighted_expert_output = expert_outputs * repeat_elements(expanded_gate_output, self.units, axis=1)\n",
    "            #print(weighted_expert_output.shape)\n",
    "            final_outputs.append(sum(weighted_expert_output, axis=2))\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers = layers.Input(shape=(499,))\n",
    "mmoe_layer = MMoE(units=4, num_experts=8, num_tasks=2)(input_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layers = []\n",
    "for index, task_layer in enumerate(mmoe_layer):\n",
    "    tower_layer = layers.Dense(units=8, activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling())(task_layer)\n",
    "    output_layer = layers.Dense(units=2, name=str(index), activation='softmax', kernel_initializer=tf.keras.initializers.VarianceScaling())(tower_layer)\n",
    "    output_layers.append(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_layers], outputs=output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 499)]             0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 499)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "m_mo_e_22 (MMoE)                [(None, 4), (None, 4 24000       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_231 (Dense)               (None, 8)            40          m_mo_e_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 8)            40          m_mo_e_22[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "income (Dense)                  (None, 2)            18          dense_231[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "marital (Dense)                 (None, 2)            18          dense_232[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,116\n",
      "Trainable params: 24,116\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(499,))#[499,]\n",
    "\n",
    "\n",
    "    # Set up MMoE layer\n",
    "mmoe_layers = MMoE(units=4, num_experts=8, num_tasks=2)(input_layer)\n",
    "\n",
    "\n",
    "output_layers = []\n",
    "output_dict = {\n",
    "    '0':'income',\n",
    "    '1':'marital'\n",
    "}\n",
    "    # Build tower layer from MMoE layer\n",
    "for index, task_layer in enumerate(mmoe_layers):\n",
    "    tower_layer = layers.Dense(units=8, activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling())(task_layer)\n",
    "    output_layer = layers.Dense(units=2, name=output_dict[str(index)], activation='softmax',kernel_initializer=tf.keras.initializers.VarianceScaling())(tower_layer)\n",
    "    output_layers.append(output_layer)\n",
    "\n",
    "    # Compile model\n",
    "model = Model(inputs=[input_layer], outputs=output_layers)\n",
    "\n",
    "    # Print out model architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    将csv文件列表处理为TFDataSet\n",
    "    @param filenames csv文件名列表\n",
    "    @param n_reader interleave cycle_length\n",
    "    @param batch_size batch_size\n",
    "    @param n_parse_threads map的并行数\n",
    "    @param shuffle_buffer_size shuffer_size\n",
    "    @return dataset TFDataset\n",
    "'''\n",
    "def csv_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename : tf.data.TextLineDataset(filename),\n",
    "        cycle_length=n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    将csv数据每一行转换为特征值和标签值\n",
    "    @param line line\n",
    "    @param n_feature 特征纬度\n",
    "    @return x 特征\n",
    "    @return y 标签\n",
    "'''\n",
    "def parse_csv_line(line, n_feature=503):\n",
    "    defs = [tf.constant(np.nan)] * n_feature\n",
    "    parsed_field = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_field[0:-4])\n",
    "    y_income = tf.stack(parsed_field[-4:-2])\n",
    "    y_marital = tf.stack(parsed_field[-2:])\n",
    "    return x, (y_income, y_marital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    通过前缀获取文件列表\n",
    "    @param source_dir 源文件目录\n",
    "    @param prefix_name 前缀名\n",
    "    @return result list\n",
    "'''\n",
    "def get_filename_by_prefix(source_dir,prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results\n",
    "source_dir='/Users/lizhen/Code/data_set/census-income/generate_csv/'\n",
    "train_filenames = get_filename_by_prefix(source_dir, 'train')\n",
    "valid_filenames = get_filename_by_prefix(source_dir, 'val')\n",
    "test_filenames = get_filename_by_prefix(source_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "            tf.keras.metrics.TruePositives(name='tp'),\n",
    "            tf.keras.metrics.FalsePositives(name='fp'),\n",
    "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.AUC(name='prauc',curve='PR')\n",
    "        ]\n",
    "model.compile(\n",
    "    loss={'income':'binary_crossentropy','marital':'binary_crossentropy'},\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=metrics,\n",
    "    experimental_run_tf_function=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = csv_reader_dataset(train_filenames)\n",
    "val_dataset = csv_reader_dataset(valid_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6236/6236 [==============================] - 241s 39ms/step - loss: 0.5481 - income_loss: 0.3852 - marital_loss: 0.1629 - income_tp: 187161.0000 - income_fp: 12362.0000 - income_tn: 187161.0000 - income_fn: 12362.0000 - income_accuracy: 0.9380 - income_precision: 0.9380 - income_recall: 0.9380 - income_auc: 0.9384 - income_prauc: 0.9152 - marital_tp: 187294.0000 - marital_fp: 12229.0000 - marital_tn: 187294.0000 - marital_fn: 12229.0000 - marital_accuracy: 0.9387 - marital_precision: 0.9387 - marital_recall: 0.9387 - marital_auc: 0.9851 - marital_prauc: 0.9848 - val_loss: 0.0000e+00 - val_income_loss: 0.0000e+00 - val_marital_loss: 0.0000e+00 - val_income_tp: 0.0000e+00 - val_income_fp: 0.0000e+00 - val_income_tn: 0.0000e+00 - val_income_fn: 0.0000e+00 - val_income_accuracy: 0.0000e+00 - val_income_precision: 0.0000e+00 - val_income_recall: 0.0000e+00 - val_income_auc: 0.0000e+00 - val_income_prauc: 0.0000e+00 - val_marital_tp: 0.0000e+00 - val_marital_fp: 0.0000e+00 - val_marital_tn: 0.0000e+00 - val_marital_fn: 0.0000e+00 - val_marital_accuracy: 0.0000e+00 - val_marital_precision: 0.0000e+00 - val_marital_recall: 0.0000e+00 - val_marital_auc: 0.0000e+00 - val_marital_prauc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6690d4690>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  4.   0.   0. ...   1.   0.   0.]\n",
      " [ 39. 500.   0. ...   1.   0.   0.]\n",
      " [ 29.   0.   0. ...   1.   0.   0.]\n",
      " ...\n",
      " [ 28.   0.   0. ...   1.   0.   0.]\n",
      " [ 17.   0.   0. ...   1.   0.   0.]\n",
      " [ 34.   0.   0. ...   1.   0.   0.]], shape=(32, 499), dtype=float32) (<tf.Tensor: id=46621, shape=(32, 2), dtype=float32, numpy=\n",
      "array([[1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.]], dtype=float32)>, <tf.Tensor: id=46622, shape=(32, 2), dtype=float32, numpy=\n",
      "array([[0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for x,y in val_dataset.take(1):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = csv_reader_dataset(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1559/Unknown - 42s 27ms/step - loss: 0.5820 - income_loss: 0.3672 - marital_loss: 0.2148 - income_tp: 46799.0000 - income_fp: 3082.0000 - income_tn: 46799.0000 - income_fn: 3082.0000 - income_accuracy: 0.9382 - income_precision: 0.9382 - income_recall: 0.9382 - income_auc: 0.9410 - income_prauc: 0.9255 - marital_tp: 45783.0000 - marital_fp: 4098.0000 - marital_tn: 45783.0000 - marital_fn: 4098.0000 - marital_accuracy: 0.9178 - marital_precision: 0.9178 - marital_recall: 0.9178 - marital_auc: 0.9737 - marital_prauc: 0.9729"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5820121931161844,\n",
       " 0.36719403,\n",
       " 0.2148183,\n",
       " 46799.0,\n",
       " 3082.0,\n",
       " 46799.0,\n",
       " 3082.0,\n",
       " 0.93821293,\n",
       " 0.93821293,\n",
       " 0.93821293,\n",
       " 0.94098943,\n",
       " 0.9255163,\n",
       " 45783.0,\n",
       " 4098.0,\n",
       " 45783.0,\n",
       " 4098.0,\n",
       " 0.9178445,\n",
       " 0.9178445,\n",
       " 0.9178445,\n",
       " 0.97368747,\n",
       " 0.9729181]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_09.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_08.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_06.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_07.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_05.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_04.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_00.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_01.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_03.csv',\n",
       " '/Users/lizhen/Code/data_set/census-income/generate_csv/test_data_02.csv']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
